# -*- coding: utf-8 -*-
"""MULTIMODAL CNN PROJECT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FjkF9zk5OA5_U6wkmTTXZHLWNnPP1mnM
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from matplotlib import pyplot as plt

# Load the dataset
url = "https://raw.githubusercontent.com/Megha1624/datasettt/main/eye%20movement.csv"
df = pd.read_csv(url)

# to describe the mean,median ,std,min of the dataset
df.describe()

#
df.head()[:5]

from matplotlib import pyplot as plt
df['AF4'].plot(kind='hist', bins=20,color = 'red', title='AF4')
plt.gca().spines[['top', 'right',]].set_visible(False)

# Adding x and y axis labels
plt.xlabel('AF4 Values')
plt.ylabel('Frequency')



from matplotlib import pyplot as plt
df['T7'].plot(kind='hist', bins=20,color = 'red', title='T7')
plt.gca().spines[['top', 'right',]].set_visible(False)
# Adding x and y axis labels
plt.xlabel('T7 Values')
plt.ylabel('Frequency')

# @title F3

from matplotlib import pyplot as plt
df['F3'].plot(kind='hist', bins=20, title='F3')
plt.gca().spines[['top', 'right',]].set_visible(False)
# Adding x and y axis labels
plt.xlabel('F3 Values')
plt.ylabel('Frequency')

from matplotlib import pyplot as plt
df['F4'].plot(kind='hist', bins=20, title='F4')
plt.gca().spines[['top', 'right',]].set_visible(False)
# Adding x and y axis labels
plt.xlabel('F4 Values')
plt.ylabel('Frequency')

df.mean(axis=1)

# In this dataset we check NULL values in the Datset

df.isnull().sum()

df['Feature1_2'] = df['AF3'] + df['F7']  # Combine two existing features
df['Feature3_4'] = df['AF4'] - df['F8']

# Split the dataset into features (X) and target variable (y)
X = df.drop('Target', axis=1)  # Features
y = df['Target']  # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize XGBoost classifier with default hyperparameters
xgb_classifier = XGBClassifier(random_state=42)

# Train the model
xgb_classifier.fit(X_train_scaled, y_train)

# Predict the labels for the test set
y_pred = xgb_classifier.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("\nXGBoost Classifier Accuracy:", accuracy)

# Print classification report
print("\nClassification Report (XGBoost Classifier):")
print(classification_report(y_test, y_pred))

# Print confusion matrix
print("\nConfusion Matrix (XGBoost Classifier):")
conf_matrix = confusion_matrix(y_test,y_pred)
print(confusion_matrix(y_test, y_pred))

import numpy as np
import itertools
from sklearn.metrics import precision_score, recall_score

# Plot confusion matrix
plt.figure(figsize=(8, 6))
plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
classes = ['Not Depressed', 'Depressed']
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation=45)
plt.yticks(tick_marks, classes)


# Labeling the plot
thresh = conf_matrix.max() / 2.
for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):
    plt.text(j, i, format(conf_matrix[i, j], 'd'),
             horizontalalignment="center",
             color="white" if conf_matrix[i, j] > thresh else "black")

plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')

# Show plot
plt.show()

# Calculate precision and recall
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

print("Precision:", precision)
print("Recall:", recall)

\

from sklearn.metrics import roc_curve, roc_auc_score

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, xgb_classifier.predict_proba(X_test_scaled)[:,1])

# Calculate AUC (Area Under the Curve)
auc = roc_auc_score(y_test, xgb_classifier.predict_proba(X_test_scaled)[:,1])

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label='ROC curve (AUC = {:.2f})'.format(auc))
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Calculate confusion matrix
conf_matrix = confusion_matrix(y_test, xgb_classifier.predict(X_test_scaled))

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

from xgboost import plot_importance

# Plot feature importance
plt.figure(figsize=(10, 8))
plot_importance(xgb_classifier, max_num_features=10)  # You can adjust the number of features to display
plt.title('Feature Importance')
plt.show()

from sklearn.model_selection import learning_curve

# Calculate learning curve
train_sizes, train_scores, test_scores = learning_curve(xgb_classifier, X_train_scaled, y_train, cv=5, scoring='accuracy', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))

# Calculate mean and standard deviation of training and test scores
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

# Plot learning curve
plt.figure(figsize=(10, 8))
plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')
plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score')
plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color='r')
plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color='g')
plt.title('Learning Curve')
plt.xlabel('Training Examples')
plt.ylabel('Score')
plt.legend(loc='best')
plt.grid(True)
plt.show()

# Scatter plot using the new features
plt.figure(figsize=(8, 6))
plt.scatter(X_train['Feature1_2'], X_train['Feature3_4'], c=y_train, cmap='viridis')
plt.xlabel('Feature 1 + Feature 2')
plt.ylabel('Feature 3 - Feature 4')
plt.title('Scatter Plot of New Features')
plt.colorbar(label='Target')
plt.show()

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, confusion_matrix, classification_report
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.model_selection import learning_curve
import seaborn as sns
import xgboost as xgb

# Assuming you have already trained your xgb_classifier and have X_test_scaled and y_test

# Calculate precision-recall curve
precision, recall, _ = precision_recall_curve(y_test, xgb_classifier.predict_proba(X_test_scaled)[:, 1])

# Plot precision-recall curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, marker='.')
plt.title('Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.grid(True)
plt.show()

# Compute confusion matrix
cm = confusion_matrix(y_test, xgb_classifier.predict(X_test_scaled))

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Plot feature importance
xgb.plot_importance(xgb_classifier, importance_type='weight', max_num_features=10, height=0.5)
plt.title('Feature Importance')
plt.show()

# Generate learning curve
train_sizes, train_scores, test_scores = learning_curve(xgb_classifier, X_train_scaled, y_train, cv=5, scoring='accuracy', n_jobs=-1)

# Calculate mean and standard deviation
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

# Plot learning curve
plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')
plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score')
plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color='r')
plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color='g')
plt.title('Learning Curve')
plt.xlabel('Training examples')
plt.ylabel('Score')
plt.legend(loc='best')
plt.grid()
plt.show()

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(y_test, xgb_classifier.predict_proba(X_test_scaled)[:, 1])

# Plot precision and recall against thresholds
plt.figure(figsize=(8, 6))
plt.plot(thresholds, precision[:-1], 'b--', label='Precision')
plt.plot(thresholds, recall[:-1], 'g-', label='Recall')
plt.title('Precision-Recall vs. Threshold')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.legend(loc='best')
plt.grid(True)
plt.show()

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, xgb_classifier.predict_proba(X_test_scaled)[:, 1])
roc_auc = roc_auc_score(y_test, xgb_classifier.predict_proba(X_test_scaled)[:, 1])

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='b', label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='best')
plt.grid(True)
plt.show()

# Bar plot for Target
plt.figure(figsize=(8, 6))
df['Target'].value_counts().plot(kind='bar', color='skyblue')
plt.xlabel('Target')
plt.ylabel('Count')
plt.title('Bar Plot of Target Distribution')
plt.xticks(rotation=0)
plt.show()

# After training your XGBoost model
import joblib

# Assuming 'xgb_model' is your trained XGBoost model
joblib.dump(xgb_classifier, 'xgb_model.pkl')

#Logistic regression for prediction the depression detection

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2']
}

# Initialize logistic regression model with increased max_iter
logistic_regression = LogisticRegression(max_iter=1000, random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=logistic_regression, param_grid=param_grid, cv=5)

# Train the model
grid_search.fit(X_train_scaled, y_train)

# Get the best parameters
best_params = grid_search.best_params_

# Train the model with the best parameters
logistic_regression_best = LogisticRegression(**best_params, random_state=42)
logistic_regression_best.fit(X_train_scaled, y_train)

# Predict the labels for the test set
y_pred_best = logistic_regression_best.predict(X_test_scaled)

# Train the model
logistic_regression.fit(X_train_scaled, y_train)

# Predict the labels for the test set
y_pred = logistic_regression.predict(X_test_scaled)

accuracy = accuracy_score(y_test, y_pred)
print("\nLogistic Regression Accuracy:", accuracy)

# Evaluate the model
accuracy_best = accuracy_score(y_test, y_pred_best)
print("\nLogistic Regression Accuracy (After Hyperparameter Tuning):", accuracy_best)

# using the  Random Forest Classifier

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler

# Initialize Random Forest classifier
random_forest = RandomForestClassifier(random_state=42)

# Train the model
random_forest.fit(X_train_scaled, y_train)

# Predict the labels for the test set
y_pred = random_forest.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("\nRandomForest Classifier Accuracy:", accuracy)

# Print classification report
print("\nClassification Report (RandomForest Classifier):")
print(classification_report(y_test, y_pred))

# Print confusion matrix
print("\nConfusion Matrix (RandomForest Classifier):")
print(confusion_matrix(y_test, y_pred))

# Print classification report
print("\nClassification Report (Random Forest Classifier):")
print(classification_report(y_test, y_pred))

# using Support vector machine

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler

# Initialize SVM classifier
svm_classifier = SVC(random_state=42)

# Train the model
svm_classifier.fit(X_train_scaled, y_train)

# Predict the labels for the test set
y_pred = svm_classifier.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("\nSVM Classifier Accuracy:", accuracy)

# Print classification report
print("\nClassification Report (SVM Classifier):")
print(classification_report(y_test, y_pred))